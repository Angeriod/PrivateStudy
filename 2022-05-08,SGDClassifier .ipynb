{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "156794f7",
   "metadata": {},
   "source": [
    "경사하강법\n",
    "\n",
    "-최적화: 오류 값을 계산하고 해당값을 최소화 하기위해 가중치변경\n",
    "-손실(cost)을 줄이는 알고리즘\n",
    "-함수의 기울기(경사)를 구하여 기울기가 낮은쪽으로 계속 이동시켜 극값(최적값)에 이를 때까지 반복하는것\n",
    "-미분값(기울기)이 최소가 되는 점을 찾아 알맞은 weight(가중치 매개변수)를 찾아냄\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf6254",
   "metadata": {},
   "source": [
    "![nn](image/Gradient1.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a761f",
   "metadata": {},
   "source": [
    "-해당 함수의최소값 위치를 찾기위해 비용함수의 경사반대 방향으로 정의한 step size를 가지고 조금씩 움직여 최적의 파라미터 찾으려는법\n",
    "-여기서 경사는 파라미터에 대해 편미분한 벡터를 의미\n",
    "-이 파라미터 반복적으로 조금씩 움직이는것이 관건"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3a094e",
   "metadata": {},
   "source": [
    "![nn](image/Gradient2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05117cad",
   "metadata": {},
   "source": [
    "-경사하강법에서는 학습시 스텝의 크기 중요\n",
    "-학습률이 너무작을수록(step size가 작을수록)\n",
    "    1. 알고리즘이 수렴하기 위해 반복해야하는 값 많으므로 학습시간 오래걸림\n",
    "    2. 지역최소값(local minimum)에 수렴할 수 있음\n",
    "-학습률이 너무 클경우(step size가 클수록)\n",
    "    1. 학습시간이 적게걸림\n",
    "    2. 스텝이 너무 커서 전역최소값(global minimum)을 가로질러 반대편으로 건너뛰어 최솨값에서 멀어질수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97d7518",
   "metadata": {},
   "source": [
    "![nn](image/Gradient3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd79b9dc",
   "metadata": {},
   "source": [
    "# 진행순서\n",
    "   #####1단계: w1에 대한 시작점 선택\n",
    "       -linear regression의 경우 위의 그림과 같이 매끈한 모양의 시작점 별로 중요 x\n",
    "       -많은 경우, w1을 0으로 설정하거나 임의의 값을 선택\n",
    "       -cost 함수가 위와 같지 않다면 시작값을 찾는것이 매우중요\n",
    "   #####2단계: 시작점에서 손실 곡선의 기울기 계산\n",
    "       -기울기=편미분의 벡터:어느방향이 더 정확한지 or 부정확한지 알려줌\n",
    "       -단일 가중치에 대한 손실의 기울기=미분값\n",
    "       -손실 함수 곡선의 다음지점을 결정하기위해 경사하강법 알고리즘은 단일 가중의 일부를 시작점으로 더함(어느 방향 +,-이동 해야하는지 결정)\n",
    "       -기울기의 보폭(Learning rate)을 통해 손실 곡선의 다음 지점으로 이동\n",
    "       3단계:위의 과정을 반복해 최소값에 접근"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538a0631",
   "metadata": {},
   "source": [
    "![nn](image/Gradient4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe465a4b",
   "metadata": {},
   "source": [
    "# 문제점\n",
    "   #####경사하강법은 현재 위치에서 기울기를 이용하기 때문에 지역최소값에서 빠질수 있음\n",
    "   무작위 초기화로 인해 알고리즘이 전역최소값이 아닌 지역최소값에 수렴할수 있음\n",
    "   평탄한 지역을 지나기위해 오랜 시간이 걸리고 일찍 멈추어서 전역  최소값에 도달하지 못할수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff8a9dc",
   "metadata": {},
   "source": [
    "![nn](image/Gradient5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b30018",
   "metadata": {},
   "source": [
    "# 해결방법:모멘텀\n",
    "   기울기에 관성을 부과하여 작은 기울기는 쉽게 넘어갈 수 있도록 만든것\n",
    "   공을 예로 들면 언덕에서 공을 굴렸을 때, 언덕은 공의 관성을 이용하여 쉽게 넘어 갈 수 있게 하여 지역 최소값을 탈출할 수 있게 한다는 뜻.\n",
    "   모멘텀을 사용하지 않으면 아주 작은 언덕에도 빠져나오지 못할 수 있으며 기울기가 매우 작은 구간을 빠져나오는데 아주 오랜 시간이 걸림."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1c56b5",
   "metadata": {},
   "source": [
    "![nn](image/Gradient6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38330151",
   "metadata": {},
   "source": [
    "# 배치사이즈\n",
    "   경사하강법에서 배치는 단일 반복에서 기울기를 계산하는데 사용하는 data의 총 개수\n",
    "   경사하강법에서의 배치는 전체 데이터 셋이라고 가정함\n",
    "   연산한번에 들어가는 데이터의 크기\n",
    "   mini batch: 1 Batch size에 해당하는 데이터셋\n",
    "   배치사이즈 너무 큼: 한번에 처리해야할 데이터 양이 많아지므로 학습속도 저하 및 메모리 부족문제\n",
    "   배치사이즈 너무 작음: 적은 데이터를 대상으로 가중치를 업데이트하고 이 업데이트가 자꾸 발생하므로 훈련이 불안정해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022c50f0",
   "metadata": {},
   "source": [
    "# 확률적 경사 하강법(SGD)\n",
    "   경사하강법은 전체 데이터를 모두 사용해서 기울기를 계산(Batch Gradient Descent)하기 때문에 학습하는데 많은 시간 필요->학습데이터 큰경우 부담\n",
    "   이러한 점을 보완하기 위해 확률적 경사하강법 이용\n",
    "   매 step에서 딱 한개의 샘플을 무작위로 선택하고 그 하나의 샘플에 대한 기울기 계산\n",
    "   배치 크기가 1인 경사하강법 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5681f9a3",
   "metadata": {},
   "source": [
    "# 특징\n",
    "   매우적은 데이터를 처리하기에 학습속도 빠름\n",
    "   하나의 샘플만 메모리에 있으면 되기 때문에 큰 데이터셋도 학습 가능\n",
    "   cost Function이 매우 불규칙할 경우 알고리즘이 local minimum을 건너뛰도록 도와주어 global minimum을 찾을 가능성 높음\n",
    "   샘플의 선택이 활률적(Stochastic)이기 떄문에 배치경사 하강법에 비해 불안정\n",
    "   cost function이 local minimun에 이를때까지 부드럽게 감소하지 않고 위아래로 요동치며 평균적으로 감소\n",
    "   반복이 충분하면 SGD가 효과 있지만 노이즈가 매우 심함\n",
    "   SGD의 여러 변형 함수의 최저점에 가까운 점을 찾을 가능성이 높지만 항상 보장되지는 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb8949",
   "metadata": {},
   "source": [
    "![nn](image/Gradient7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9420a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
