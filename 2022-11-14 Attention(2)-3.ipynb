{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2917a801",
   "metadata": {},
   "source": [
    "https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f618d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c75f248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1234\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d587e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de=spacy.load('de_core_news_sm')\n",
    "spacy_en=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a68a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c79bf415",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC=Field(tokenize=tokenize_de, init_token='', eos_token='',lower=True)\n",
    "TRG=Field(tokenize=tokenize_en, init_token='',eos_token='',lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "909a4367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading training.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lee_JongWoo\\private\\20221115\\.data\\multi30k\\training.tar.gz: 100%|████████| 1.21M/1.21M [00:06<00:00, 194kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading validation.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lee_JongWoo\\private\\20221115\\.data\\multi30k\\validation.tar.gz: 100%|█████| 46.3k/46.3k [00:00<00:00, 64.0kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading mmt_task1_test2016.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lee_JongWoo\\private\\20221115\\.data\\multi30k\\mmt_task1_test2016.tar.gz: 100%|█| 66.2k/66.2k [00:00<00:00, 94.3k\n"
     ]
    }
   ],
   "source": [
    "train_data,valid_data,test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC,TRG))#exts는 source와 target으로 사용할 언어를 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd81727d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "8\n",
      "{'src': ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n",
      "{'src': ['mehrere', 'männer', 'mit', 'schutzhelmen', 'bedienen', 'ein', 'antriebsradsystem', '.'], 'trg': ['several', 'men', 'in', 'hard', 'hats', 'are', 'operating', 'a', 'giant', 'pulley', 'system', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(len(vars(train_data.examples[0])['src']))\n",
    "print(len(vars(train_data.examples[1])['src']))\n",
    "\n",
    "print(vars(train_data.examples[0]))\n",
    "print(vars(train_data.examples[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "def59ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)#build_vocab함수를 이용하여 각 token을 indexing해줌. 이때, source와 target의 vocabulary는 다름\n",
    "TRG.build_vocab(train_data, min_freq=2)#이때, vocabulary는 training set에서만 만들어져야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76422d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fac104cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device)\n",
    "#BucketIterator를 이용해 batch size별로 token묶고 어휘를 읽은수 있는 token에서 index로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e73b9857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4284\n",
      "첫번째 배치의 text크기:torch.Size([128, 23])\n",
      "tensor([  2,   7,  15,   6,  13, 330, 255,  30,  11,  23, 267,   3,   2,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1])\n",
      "tensor([   2,   53, 1550,    8,   34,    4,  307,   10,   76,   86,   21,   13,\n",
      "         500,   61,    3,    2,    1,    1,    1,    1,    1,    1,    1])\n",
      "227\n",
      "29056\n"
     ]
    }
   ],
   "source": [
    "print(TRG.vocab.stoi['pitched'])#의 index=1\n",
    "\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    src=batch.src\n",
    "    trg=batch.trg\n",
    "    \n",
    "    src=src.transpose(1,0)\n",
    "    print(f\"첫번째 배치의 text크기:{src.shape}\")\n",
    "    print(src[0])\n",
    "    print(src[1])\n",
    "    \n",
    "    break\n",
    "\n",
    "        \n",
    "print(len(train_iterator))\n",
    "print(len(train_iterator)*128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2122c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim,emb_dim)\n",
    "        #양방향=True\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True) #ht=EncoderGRU^->(e(xt^->),ht-1^->) 두개 넣음.\n",
    "         # 양방향 rnn의 출력값을 concat 한 후에 fc layer에 전달합니다.\n",
    "        self.fc=nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    def forward(self,src):\n",
    "        #src=[src len, batch_size]\n",
    "        embedded= self.dropout(self.embedding(src))\n",
    "        #embedded = [src len, batch_size, emb dim]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        #outputs=[단어길이, 배치사이즈, 은닉차원 * num_directions] 이건 time step마다 나오는 outputs임 //output은 언제나 hidden layer의 top에 있음\n",
    "        #hidden=[n_layers*num_direction(2), batch size, hid_dim]\n",
    "        #hidden layer는 [forward1,backward1,forward2,backward2... 식으로 쌓임]\n",
    "        #hidden[-2,:,:]--> 마지막 forward, hidden[-1,:,:]->마지막 backward //hidden: 마지막 은닉상태 ht값임.\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90731483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) +dec_hid_dim, dec_hid_dim) #attn= 얘는뭘까 in= (enc_hid_dim * 2) +dec_hid_dim, out=dec_hid_dim ?\n",
    "        #Energy 계산시 attn에 st-1과 H가 들어감. \n",
    "        self.v = nn.Linear(dec_hid_dim,1, bias=False) #[1,dec_hid_dim] tensor ^at=vEt\n",
    "    \n",
    "    def forward(self,hidden,encoder_outputs):\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoderoutput=(모든 h들) [src len,batchsize,enc_hid_dim*2]\n",
    "        batch_size=encoder_outputs.shape[1]\n",
    "        src_len=encoder_outputs.shape[0]\n",
    "        \n",
    "        hidden=hidden.unsqueeze(1).repeat(1, src_len, 1)#unsqueeze dim1에 1인차원채우기 torch.repeat(*sizes)\n",
    "        \n",
    "        encoder_outputs=encoder_outputs.permute(1,0,2)#차원의 자리만 바꿈 (0,1,2)->(1,0,2) \n",
    "        #hidden = [batchsize, src_len, dec_hid_dim]\n",
    "        #encoder=[bathsize, src_len, enc_hid_dim*2]\n",
    "        #위의 두코드는 아마 차원맞추기 위해서 짠듯 cat할려고 나중에 print로 한번 넣어보면 알듯?\n",
    "        \n",
    "        energy=torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))# Et=tanh(attn(st-1,H))\n",
    "        #energy=[bath_size, src_len,dec_hid_dim]-->맞나? 나중에 print 넣어서 shape 보기[(enc_hid_dim*2)+dec_hid dim 왜아님]\n",
    "        \n",
    "        attention=self.v(energy).squeeze(2)# at=vEt로 a 얘도 차원봐야알듯\n",
    "        #attention=[batch size, src len]\n",
    "        return F.softmax(attention, dim=1)#A 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0eecb8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim= output_dim\n",
    "        \n",
    "        self.attention=attention\n",
    "        \n",
    "        self.embedding=nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim*2) + emb_dim,dec_hid_dim)##rnn_input=torch.cat((embedded, weighted), dim=2) 하단에\n",
    "        \n",
    "        self.fc_out=nn.Linear((enc_hid_dim*2)+dec_hid_dim + emb_dim, output_dim)\n",
    "        ###prediction=[batchsize,output_dim=(end_hid_dim*2]+dec_hid_dim+emb_dim)]\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        #input=[batch size]\n",
    "        #hidden=[batch size, dec_hid_dim]\n",
    "        #eocnder_outputs=[src len, batch size, end hid dim*2]\n",
    "        \n",
    "        input= input.unsqueeze(0)\n",
    "        #input =[1, batch size]\n",
    "        embedded=self.dropout(self.embedding(input))\n",
    "        #embedded = [1,batch size, emd_dim]\n",
    "        a=self.attention(hidden, encoder_outputs)\n",
    "        # 위에 Attention 클래스에 넣는 것과 동일 \n",
    "        #a = [batch size, src len]\n",
    "        a=a.unsqueeze(1)\n",
    "        #a= [batch size, 1, src len]\n",
    "        encoder_outputs = encoder_outputs.permute(1,0,2)\n",
    "        #encoder_outputs = [batch size, src len, end_hid dim*2]\n",
    "        weighted=torch.bmm(a, encoder_outputs)# bmm [Batch, n, m] x [Batch,m,p]=[Batch,n,p] 배치는 나두고 내적\n",
    "        #weighted=[batch size, 1, end_hid_dim*2 w=atxH\n",
    "        weighted=weighted.permute(1,0,2)\n",
    "        #weighted=[1,batch size, end_hid_dim*2]\n",
    "        rnn_input=torch.cat((embedded, weighted), dim=2)\n",
    "        ##rnn_input=[1,batch_size,emb_dim+(enc_hid_dim)*2]\n",
    "        output, hidden=self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        #output=[src len, batch_size, dec_hid_dim * n_directions(1->단방향)] 디코더에서 n_directions=1\n",
    "        #hidden=[n_layers*n_directions(1*1),batch_size,dec hid dim] 디코더에서 n_layers는 1\n",
    "        assert (output == hidden).all()\n",
    "        #this also means that output == hidden\n",
    "        embedded = embedded.squeeze(0)\n",
    "        #embedded=[batch_size,emb_dim]\n",
    "        output=output.squeeze(0)\n",
    "        #output=[batch_size,dec_hid_dim*n_directions]\n",
    "        weighted=weighted.squeeze(0)\n",
    "        #weight=[batch_size,end_hid_dim*2]\n",
    "        prediction=self.fc_out(torch.cat((output, weighted, embedded),dim=1))\n",
    "        ###prediction=[batchsize,output_dim=(end_hid_dim*2]+dec_hid_dim+emb_dim)]\n",
    "        return prediction, hidden.squeeze(0) #decoder에서는 y와 각gru에서나온 hidden state 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38bfb24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "        self.device=device\n",
    "    \n",
    "    def forward(self,src,trg,teacher_forcing_ratio=0.5):\n",
    "        \n",
    "        #src=[src len, batch size]\n",
    "        #trg=[trg len, batch size]\n",
    "        \n",
    "        batch_size=src.shape[1]\n",
    "        trg_len=trg.shape[0]\n",
    "        \n",
    "        outputs= torch.zeros(trg_len, bath_size, trg_vocab_size).to(self.device)\n",
    "        #enc_output 은 H중에 forward와 backward 들의 모임\n",
    "        #hidden 은 마지막 forward 와 backwad h\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        input=trg[0,:] #첫번째 trg 입력값은 토큰<sos>인듯 \n",
    "        \n",
    "        for t in range (1, trg_len):\n",
    "            #디코더에 token_embedding, 이전 hidden state,H 넣기\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            \n",
    "            #각각 토큰에 대한 predcition outputs에 넣기\n",
    "            outputs[t]=output\n",
    "            #teacher force 임계값 정하기\n",
    "            teacher_force =random.random()<teacher_forcing_ratio\n",
    "            #우리 예측에서 가장 높은 prediction token 구하기\n",
    "            top1=output.argmax(1)\n",
    "            #teacher force시, 실제 다음 토큰을 다음 input으로써 사용\n",
    "            #아니라면 predicted token 사용\n",
    "            input=trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24a48d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM= len(TRG.vocab)\n",
    "ENC_EMB_DIM=256\n",
    "DEC_EMB_DIM=256\n",
    "ENC_HID_DIM=512\n",
    "DEC_HID_DIM=512\n",
    "ENC_DROPOUT=0.5\n",
    "DEC_DROPOUT=0.5\n",
    "\n",
    "attn=Attention(ENC_HID_DIM,DEC_HID_DIM)\n",
    "enc=Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec=Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model=Seq2Seq(enc, dec, attn).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1c8389b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7852, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(5892, 256)\n",
       "    (rnn): GRU(1280, 512)\n",
       "    (fc_out): Linear(in_features=1792, out_features=5892, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (device): Attention(\n",
       "    (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
       "    (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모든 bias=0 모든 weights N(0,0.01)\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data,0)\n",
    "    \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0bb7a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 20,516,100 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77b27b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bc2ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG_PAD_IDX= TRG.vocab.stoi[TRG.pad_token]\n",
    "criterion=nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)#pad들어간건 무시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f564fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss=0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        src=batch.src\n",
    "        trg=batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output=model(src,trg)\n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        output_dim=output.shape[-1]\n",
    "        \n",
    "        output=output[1:].view(-1, output_dim)\n",
    "        trg=trg[1:].view(-1)\n",
    "        #trg=[(trg len-1)*batch_size]\n",
    "        #output=[(trg len-1)*batch_size, output_dim]\n",
    "        \n",
    "        loss=criterion(output,trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss+=epoch.itme()\n",
    "        \n",
    "    return epoch_loss/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa024294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            src=batch.src\n",
    "            trg=batch.trg\n",
    "            \n",
    "            output=model(src,trg,0)#teaching force 정지\n",
    "            \n",
    "            output_dim=output.shape[-1]\n",
    "            \n",
    "            output=output[1:].view(-1, output_dim)\n",
    "            trg=trg[1:].view(-1)\n",
    "            \n",
    "            loss=criterion(output,trg)\n",
    "            \n",
    "            epoch_loss+=loss.item()\n",
    "            \n",
    "        return epoch_loss/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c37c4148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time=end_time-start_time\n",
    "    elapsed_mins=int(elapsed_time/60)\n",
    "    elapsed_secs=int(elapsed_time- (elapsed_mins*60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc2053",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS=10\n",
    "CLIP=1\n",
    "best_valid_loss= int('inf')\n",
    "\n",
    "for "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
