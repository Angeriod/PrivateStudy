{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "309136db",
   "metadata": {},
   "source": [
    "https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb2e8bf",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1409.0473.pdf?utm_source=ColumnsChannel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44db1243",
   "metadata": {},
   "source": [
    "https://happy-jihye.github.io/nlp/nlp-7/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "542632f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49aad285",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1234\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a012c8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de=spacy.load('de_core_news_sm')\n",
    "spacy_en=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a5147e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1414f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC=Field(tokenize=tokenize_de, init_token='', eos_token='',lower=True)\n",
    "TRG=Field(tokenize=tokenize_en, init_token='',eos_token='',lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f25cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,valid_data,test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC,TRG))#exts는 source와 target으로 사용할 언어를 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6f642be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "8\n",
      "{'src': ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n",
      "{'src': ['mehrere', 'männer', 'mit', 'schutzhelmen', 'bedienen', 'ein', 'antriebsradsystem', '.'], 'trg': ['several', 'men', 'in', 'hard', 'hats', 'are', 'operating', 'a', 'giant', 'pulley', 'system', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(len(vars(train_data.examples[0])['src']))\n",
    "print(len(vars(train_data.examples[1])['src']))\n",
    "\n",
    "print(vars(train_data.examples[0]))\n",
    "print(vars(train_data.examples[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31bf8c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)#build_vocab함수를 이용하여 각 token을 indexing해줌. 이때, source와 target의 vocabulary는 다름\n",
    "TRG.build_vocab(train_data, min_freq=2)#이때, vocabulary는 training set에서만 만들어져야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d008229",
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26e88af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device)\n",
    "#BucketIterator를 이용해 batch size별로 token묶고 어휘를 읽은수 있는 token에서 index로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e04ebcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4284\n",
      "첫번째 배치의 text크기:torch.Size([128, 36])\n",
      "tensor([  2,   4,  12, 157,   8,   6,  13, 339,  33, 592,  27, 694,   3,   2,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1])\n",
      "tensor([   2,    4,  958,   19, 4972,  211,    6,  196,  951,    3,    2,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1])\n",
      "227\n",
      "29056\n"
     ]
    }
   ],
   "source": [
    "print(TRG.vocab.stoi['pitched'])#<pad>의 index=1\n",
    "\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    src=batch.src\n",
    "    trg=batch.trg\n",
    "    \n",
    "    src=src.transpose(1,0)\n",
    "    print(f\"첫번째 배치의 text크기:{src.shape}\")\n",
    "    print(src[0])\n",
    "    print(src[1])\n",
    "    \n",
    "    break\n",
    "\n",
    "        \n",
    "print(len(train_iterator))\n",
    "print(len(train_iterator)*128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07409ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embdim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim,emb_dim)\n",
    "        #양방향=True\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True) #ht=EncoderGRU^->(e(xt^->),ht-1^->) 두개 넣음.\n",
    "         # 양방향 rnn의 출력값을 concat 한 후에 fc layer에 전달합니다.\n",
    "        self.fc=nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    def forward(self,src):\n",
    "        #src=[src len, batch_size]\n",
    "        embedded= self.dropout(self.embedding(src))\n",
    "        #embedded = [src len, batch_size, emb dim]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        #outputs=[단어길이, 배치사이즈, 은닉차원 * num_directions] 이건 time step마다 나오는 outputs임 //output은 언제나 hidden layer의 top에 있음\n",
    "        #hidden=[n_layers*num_direction(2), batch size, hid_dim]\n",
    "        #hidden layer는 [forward1,backward1,forward2,backward2... 식으로 쌓임]\n",
    "        #hidden[-2,:,:]--> 마지막 forward, hidden[-1,:,:]->마지막 backward //hidden: 마지막 은닉상태 ht값임.\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac1a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) +dec_hid_dim, dec_hid_dim) #attn= 얘는뭘까 in= (enc_hid_dim * 2) +dec_hid_dim, out=dec_hid_dim ?\n",
    "        #Energy 계산시 attn에 st-1과 H가 들어감. \n",
    "        self.v = nn.Linear(dec_hid_dim,1, bias=False) #[1,dec_hid_dim] tensor ^at=vEt\n",
    "    \n",
    "    def forward(self,hidden,encoder_outputs):\n",
    "        #encoderoutput=(모든 h들) [src len,batchsize,enc_hid_dim*2]\n",
    "        batch_size=encoder_outputs.shape[1]\n",
    "        src_len=encoder_outputs.shape[0]\n",
    "        \n",
    "        hidden=hidden.unsqueeze(1).repeat(1, src_len, 1)#unsqueeze dim1에 1인차원채우기 torch.repeat(*sizes)\n",
    "        \n",
    "        encoder_outputs=encoder_outputs.permute(1,0,2)#차원의 자리만 바꿈 (0,1,2)->(1,0,2) \n",
    "        #위의 두코드는 아마 차원맞추기 위해서 짠듯 cat할려고 나중에 print로 한번 넣어보면 알듯?\n",
    "        \n",
    "        energy=torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))# Et=tanh(attn(st-1,H))\n",
    "        \n",
    "        attention=self.v(energy).squeeze(2)# at=vEt로 a 얘도 차원봐야알듯\n",
    "        \n",
    "        return F.softmax(attention, dim=1)#A 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa58e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim= output_dim\n",
    "        self.attention=attention\n",
    "        self.embedding=nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim*2) + emb_dim,dec_hid_dim)\n",
    "        self.fc_out=nn.Linear((enc_hid_dim*2)+dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \n",
    "        input= input.unsqueeze(0)\n",
    "        \n",
    "        embedded=self.dropout(self.embedding(input))\n",
    "        \n",
    "        a=self.attention(hidden, encoder_outputs)\n",
    "        \n",
    "        a=a.unsqueeze(1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1,0,2)\n",
    "        \n",
    "        weighted=torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        weighted=weighted.permute(1,0,2)\n",
    "        \n",
    "        rnn_input=torch.cat((embedded, weighted), dim=2)\n",
    "        \n",
    "        output, hidden=self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output=output.squeeze(0)\n",
    "        weighted=weighted.squeeze(0)\n",
    "        \n",
    "        prediction=self.fc_out(torch.cat((output, weighted, embedded),dim=1))\n",
    "        \n",
    "        return prediction, hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c83617c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
